{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db823349-554f-4e1c-8b9f-47a4503d8fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Stock_Dataset.ipynb\n",
      "importing Jupyter notebook from AttLSTM.ipynb\n",
      "importing Jupyter notebook from attention.ipynb\n",
      "importing Jupyter notebook from Transformer_Encoder.ipynb\n",
      "importing Jupyter notebook from metric.ipynb\n",
      "importing Jupyter notebook from loss_fn.ipynb\n",
      "importing Jupyter notebook from Stock_datasets_csv.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append('C:\\\\taewon_project\\\\DTML_selfatt')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from Stock_Dataset import StockDataset\n",
    "import argparse\n",
    "from AttLSTM import att_LSTM\n",
    "from Transformer_Encoder import Transformer\n",
    "import numpy as np\n",
    "import time\n",
    "from metric import metric_acc as ACC\n",
    "from metric import metric_mcc as MCC\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "from loss_fn import Selective_Regularization\n",
    "from Stock_datasets_csv import stock_csv_read\n",
    "\n",
    "def train(att_LSTM,transformer,                                  ## Model\n",
    "          att_LSTM_optimizer, transformer_optimizer,   ## Optimizer\n",
    "          Partition, args):                                      ## Data, loss function, argument\n",
    "    trainloader = DataLoader(Partition['train'],\n",
    "                             batch_size = args.batch_size,\n",
    "                             shuffle=False, drop_last=True)\n",
    "\n",
    "    att_LSTM.train()\n",
    "    transformer.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for j, data in enumerate(trainloader):\n",
    "        data_out_list = []\n",
    "        for i in range(len(data)):\n",
    "\n",
    "            att_LSTM_optimizer.zero_grad()\n",
    "            transformer_optimizer.zero_grad()\n",
    "            \n",
    "            x_input = data[i][0].to(args.device)\n",
    "            \n",
    "            if i == 1:\n",
    "                true_y = data[i][1].squeeze().float().to(args.device)\n",
    "            \n",
    "            att_LSTM.hidden = [hidden.to(args.device) for hidden in att_LSTM.init_hidden()]\n",
    "            \n",
    "            # 'list' object has no attribute 'float'\n",
    "\n",
    "            hidden_context, attn_weights = att_LSTM(x_input)\n",
    "\n",
    "            data_out_list.append(hidden_context)\n",
    "\n",
    "        index_output = data_out_list[0]  # torch.Size([128, 10])\n",
    "        stock_output = data_out_list[1] # torch.Size([128, 10])\n",
    "        \n",
    "        # torch.Size([64, 10, 10]) batch seq fea\n",
    "        Transformer_input = index_output* args.market_beta + stock_output\n",
    "        \n",
    "        \n",
    "        output1,wp,bp = transformer(Transformer_input.transpose(0,1))\n",
    "\n",
    "\n",
    "        # output_ = torch.where(output1 >= 0.5, 1.0, 0.0)\n",
    "        # output_.requires_grad=True\n",
    "        \n",
    "        loss = args.loss_fn(output1, true_y, wp.squeeze(), bp)\n",
    "        loss.backward()\n",
    "\n",
    "        att_LSTM_optimizer.step() ## parameter 갱신\n",
    "        transformer_optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    return att_LSTM, transformer, train_loss\n",
    "\n",
    "\n",
    "def validation(att_LSTM,transformer,\n",
    "               partition, args):\n",
    "    valloader = DataLoader(partition['val'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    att_LSTM.eval()\n",
    "    transformer.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j, data in enumerate(valloader):\n",
    "\n",
    "            data_out_list = []\n",
    "            for i in range(len(data)):\n",
    "                \n",
    "                x_input = data[i][0].to(args.device)\n",
    "\n",
    "                if i == 1:\n",
    "                    true_y = data[i][1].squeeze().float().to(args.device)\n",
    "                    \n",
    "                att_LSTM.hidden = [hidden.to(args.device) for hidden in att_LSTM.init_hidden()]\n",
    "\n",
    "\n",
    "                hidden_context, attn_weights = att_LSTM(x_input)\n",
    "\n",
    "                data_out_list.append(hidden_context)\n",
    "\n",
    "            index_output = data_out_list[0]  # torch.Size([128, 10])\n",
    "            stock_output = data_out_list[1]  # torch.Size([128, 10])\n",
    "\n",
    "\n",
    "            Transformer_input = index_output* args.market_beta  + stock_output\n",
    "\n",
    "            output1,wp,bp = transformer(Transformer_input.transpose(0,1))\n",
    "\n",
    "            loss = args.loss_fn(output1, true_y, wp.squeeze(), bp)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss = val_loss / len(valloader)\n",
    "        return att_LSTM, transformer, val_loss\n",
    "\n",
    "\n",
    "def test(att_LSTM, transformer,\n",
    "               partition, args):\n",
    "    testloader = DataLoader(partition['test'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    att_LSTM.eval()\n",
    "    transformer.eval()\n",
    "\n",
    "    ACC_metric = 0.0\n",
    "    MCC_metric = 0.0\n",
    "    with torch.no_grad():\n",
    "        for j, data in enumerate(testloader):\n",
    "\n",
    "            data_out_list = []\n",
    "            for i in range(len(data)):\n",
    "                \n",
    "                x_input = data[i][0].to(args.device)\n",
    "\n",
    "                if i == 1:\n",
    "                    true_y = data[i][1].squeeze().float().to(args.device)\n",
    "\n",
    "                att_LSTM.hidden = [hidden.to(args.device) for hidden in att_LSTM.init_hidden()]\n",
    "\n",
    "\n",
    "                hidden_context, attn_weights = att_LSTM(x_input)\n",
    "\n",
    "                data_out_list.append(hidden_context)\n",
    "\n",
    "            index_output = data_out_list[0]  # torch.Size([128, 10])\n",
    "            stock_output = data_out_list[1]  # torch.Size([128, 10])\n",
    "\n",
    "\n",
    "            Transformer_input = index_output * args.market_beta + stock_output\n",
    "\n",
    "            output1,wp,bp = transformer(Transformer_input.transpose(0,1))\n",
    "\n",
    "            output_ = torch.where(output1 >= 0.5, 1.0, 0.0)\n",
    "\n",
    "            output_.requires_grad = True\n",
    "\n",
    "            ACC_metric += ACC(output_, true_y)\n",
    "            MCC_metric += MCC(output_, true_y)\n",
    "\n",
    "        ACC_metric = ACC_metric / len(testloader)\n",
    "        MCC_metric = MCC_metric / len(testloader)\n",
    "\n",
    "        return ACC_metric, MCC_metric\n",
    "\n",
    "\n",
    "\n",
    "def experiment(partition, args):\n",
    "    att_LSTM = args.att_LSTM(args.input_dim, args.hid_dim, args.output_dim, args.num_layers, args.batch_size, args.dropout, args.use_bn, args.attention_head, args.attn_size, args.activation)\n",
    "    transformer = args.transformer(args.trans_feature_size, args.trans_num_laysers, args.dropout, args.batch_size, args.x_frames, args.trans_nhead)\n",
    "    \n",
    "    att_LSTM.to(args.device)\n",
    "    transformer.to(args.device)\n",
    "\n",
    "    if args.optim == 'SGD':\n",
    "        att_LSTM_optimizer = optim.SGD(att_LSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "        transformer_optimizer = optim.SGD(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        att_LSTM_optimizer = optim.RMSprop(att_LSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "        transformer_optimizer = optim.RMSprop(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        att_LSTM_optimizer = optim.Adam(att_LSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "        transformer_optimizer = optim.Adam(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "\n",
    "    # ===== List for epoch-wise data ====== #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    # ===================================== #\n",
    "    for epoch in range(args.epoch):\n",
    "        ts = time.time()\n",
    "        att_LSTM, transformer, train_loss = train(att_LSTM, transformer,\n",
    "                                                  att_LSTM_optimizer, transformer_optimizer,\n",
    "                                                  partition, args)\n",
    "\n",
    "        att_LSTM, transformer, val_loss = validation(att_LSTM, transformer, partition, args)\n",
    "\n",
    "        te = time.time()\n",
    "\n",
    "        ## 각 에폭마다 모델을 저장하기 위한 코드\n",
    "        torch.save(att_LSTM.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_epoch' +'_att_LSTM' +'.pt')\n",
    "        torch.save(transformer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_epoch' +'_transformer' +'.pt')\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print('Epoch {}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "              .format(epoch, train_loss, val_loss, te - ts))\n",
    "\n",
    "    ## val_losses에서 가장 값이 최소인 위치를 저장함\n",
    "    site_val_losses = val_losses.index(min(val_losses)) ## 10 epoch일 경우 0번째~9번째 까지로 나옴\n",
    "    att_LSTM = args.att_LSTM(args.input_dim, args.hid_dim, args.output_dim, args.num_layers, args.batch_size, args.dropout, args.use_bn, args.attention_head, args.attn_size, args.activation)\n",
    "    transformer = args.transformer(args.trans_feature_size, args.trans_num_laysers,args.dropout, args.batch_size, args.x_frames, args.trans_nhead)\n",
    "    att_LSTM.to(args.device)\n",
    "    transformer.to(args.device)\n",
    "\n",
    "    att_LSTM.load_state_dict(torch.load(args.split_file_path + '\\\\' + str(site_val_losses) +'_epoch' +'_att_LSTM'+ '.pt'))\n",
    "    transformer.load_state_dict(torch.load(args.split_file_path + '\\\\' + str(site_val_losses) + '_epoch' + '_transformer' + '.pt'))\n",
    "\n",
    "    ACC,MCC = test(att_LSTM, transformer, partition, args)\n",
    "    print('ACC: {}, MCC: {}'.format(ACC, MCC))\n",
    "\n",
    "    with open(args.split_file_path + '\\\\'+ str(site_val_losses)+'Epoch_test_metric' +'.csv', 'w') as fd:\n",
    "        print('ACC: {}, MCC: {}'.format(ACC, MCC), file=fd)\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['ACC'] = ACC\n",
    "    result['MCC'] = MCC\n",
    "\n",
    "    return vars(args), result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd115972-567d-475b-ac08-15dedfc7d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# ========= experiment setting ========== #\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.save_file_path = \"c:\\\\taewon_project\\\\DTML_selfatt\\\\results\"\n",
    "\n",
    "\n",
    "# ====== hyperparameter ======= #\n",
    "args.batch_size = 64\n",
    "args.x_frames = 10\n",
    "args.y_frames = 1\n",
    "args.input_dim = 10\n",
    "args.output_dim = 1\n",
    "args.dropout = 0.2\n",
    "args.use_bn = True\n",
    "args.loss_fn = Selective_Regularization  ## loss function for classification : cross entropy\n",
    "args.optim = 'Adam'\n",
    "args.lr = 0.0005\n",
    "args.l2 = 0.00001 #?\n",
    "args.epoch = 100\n",
    "args.activation=\"ReLU\"\n",
    "\n",
    "# ============= model ================== #\n",
    "args.att_LSTM = att_LSTM\n",
    "args.transformer = Transformer\n",
    "\n",
    "# ====== att_lstm hyperparameter ======= #\n",
    "args.hid_dim = 10\n",
    "args.attention_head = 1\n",
    "args.attn_size = 10\n",
    "args.num_layers = 1\n",
    "args.decoder_x_frames = 1\n",
    "\n",
    "# ====== transformer hyperparameter ======= #\n",
    "args.trans_feature_size = 40\n",
    "args.trans_num_laysers = 1\n",
    "args.trans_nhead = 10\n",
    "args.market_beta = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fdb0bdf-b811-4e2c-a9a4-1289b194c7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss(train/val) 4.88525/4.88016. Took 0.45 sec\n",
      "Epoch 1, Loss(train/val) 4.87988/4.88194. Took 0.65 sec\n",
      "Epoch 2, Loss(train/val) 4.87837/4.88276. Took 0.54 sec\n",
      "Epoch 3, Loss(train/val) 4.87981/4.88282. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 4.87951/4.88265. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 4.87900/4.88284. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 4.87785/4.88348. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 4.87869/4.88289. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 4.87805/4.88292. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 4.87674/4.88355. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 4.87813/4.88428. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 4.87885/4.88477. Took 0.51 sec\n",
      "Epoch 12, Loss(train/val) 4.87907/4.88400. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 4.87784/4.88347. Took 0.48 sec\n",
      "Epoch 14, Loss(train/val) 4.87778/4.88441. Took 0.48 sec\n",
      "Epoch 15, Loss(train/val) 4.87902/4.88504. Took 0.56 sec\n",
      "Epoch 16, Loss(train/val) 4.87770/4.88537. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 4.87835/4.88469. Took 0.48 sec\n",
      "Epoch 18, Loss(train/val) 4.87644/4.88431. Took 0.47 sec\n",
      "Epoch 19, Loss(train/val) 4.87758/4.88519. Took 0.47 sec\n",
      "Epoch 20, Loss(train/val) 4.87711/4.88430. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 4.87737/4.88439. Took 0.49 sec\n",
      "Epoch 22, Loss(train/val) 4.87746/4.88384. Took 0.48 sec\n",
      "Epoch 23, Loss(train/val) 4.87514/4.88491. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 4.87769/4.88586. Took 0.48 sec\n",
      "Epoch 25, Loss(train/val) 4.87683/4.88455. Took 0.48 sec\n",
      "Epoch 26, Loss(train/val) 4.87742/4.88438. Took 0.49 sec\n",
      "Epoch 27, Loss(train/val) 4.87555/4.88284. Took 0.52 sec\n",
      "Epoch 28, Loss(train/val) 4.87474/4.88460. Took 0.57 sec\n",
      "Epoch 29, Loss(train/val) 4.87396/4.88257. Took 0.50 sec\n",
      "Epoch 30, Loss(train/val) 4.87498/4.88118. Took 0.51 sec\n",
      "Epoch 31, Loss(train/val) 4.87378/4.88338. Took 0.57 sec\n",
      "Epoch 32, Loss(train/val) 4.87284/4.88244. Took 0.50 sec\n",
      "Epoch 33, Loss(train/val) 4.87338/4.88105. Took 0.56 sec\n",
      "Epoch 34, Loss(train/val) 4.87006/4.88341. Took 0.48 sec\n",
      "Epoch 35, Loss(train/val) 4.87876/4.88200. Took 0.48 sec\n",
      "Epoch 36, Loss(train/val) 4.87538/4.88297. Took 0.51 sec\n"
     ]
    }
   ],
   "source": [
    "## 실행 파일\n",
    "args.data_list = os.listdir(\"C:\\\\taewon_project\\\\DTML_selfatt\\\\data\\\\kdd17\\\\ourpped\")\n",
    "\n",
    "\n",
    "with open(args.save_file_path + '\\\\' + 'DTML_result_t.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"model\", \"stock\", \"entire_exp_time\",  \"avg_test_ACC\", \"avg_test_ACC_std\", \"avg_test_MCC\"])\n",
    "\n",
    "    for data in args.data_list:\n",
    "        \n",
    "        stock = data.split('.')[0]\n",
    "\n",
    "        est = time.time()\n",
    "        setattr(args, 'symbol', stock)\n",
    "        args.new_file_path = args.save_file_path + '\\\\' + \"DTML_\" + args.symbol\n",
    "        os.makedirs(args.new_file_path)\n",
    "        \n",
    "        \n",
    "        csv_read = stock_csv_read(data,args.x_frames,args.y_frames)\n",
    "        split_data_list = csv_read.cv_split()\n",
    "        \n",
    "        ACC_cv = []\n",
    "        for i, data in enumerate(split_data_list):\n",
    "            args.split_file_path = args.new_file_path + \"\\\\\" + str(i) +\"th_iter\"\n",
    "            os.makedirs(args.split_file_path)\n",
    "            # 0번째에 index 1번째에 stock 1개가 input으로 들어감\n",
    "            trainset = StockDataset(data[0])\n",
    "            valset = StockDataset(data[1])\n",
    "            testset = StockDataset(data[2])\n",
    "        \n",
    "\n",
    "            partition = {'train': trainset, 'val': valset, 'test': testset}\n",
    "\n",
    "\n",
    "            setting, result = experiment(partition, args)\n",
    "            eet = time.time()\n",
    "            entire_exp_time = eet - est\n",
    "\n",
    "            fig = plt.figure()\n",
    "            plt.plot(result['train_losses'])\n",
    "            plt.plot(result['val_losses'])\n",
    "            plt.legend(['train_losses', 'val_losses'], fontsize=15)\n",
    "            plt.xlabel('epoch', fontsize=15)\n",
    "            plt.ylabel('loss', fontsize=15)\n",
    "            plt.grid()\n",
    "            plt.savefig(args.split_file_path + '\\\\' + str(args.symbol) + '_fig' + '.png')\n",
    "            plt.close(fig)\n",
    "            ACC_cv.append(result['ACC'])\n",
    "            # csv파일에 기록하기\n",
    "        ACC_cv_ar = np.array(ACC_cv)\n",
    "        acc_avg = np.mean(ACC_cv_ar)\n",
    "        acc_std = np.std(ACC_cv_ar)\n",
    "\n",
    "        wr.writerow([\"DTML\", args.symbol, entire_exp_time, acc_avg, acc_std, result['MCC']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4bb3f-2033-4b97-8031-19370a89d75b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2982973059bb32b2c180859bcc6f8f2be9d4decc2aea8640d27e050b49ac75fb"
  },
  "kernelspec": {
   "display_name": "py38_64",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
