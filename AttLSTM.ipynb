{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "650a3f6d-1839-43f9-aeb9-3a995d6e31b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from attention.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import attention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class att_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, batch_size, dropout, use_bn, attn_head, attn_size, activation):\n",
    "        super(att_LSTM,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn\n",
    "        self.attn_head = attn_head\n",
    "        self.attn_size = attn_size\n",
    "\n",
    "        self.activation = getattr(nn, activation)()\n",
    "\n",
    "        ## 파이토치에 있는 lstm모듈\n",
    "        ## output dim 은 self.regressor에서 사용됨\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "\n",
    "        self.attention = attention.Attention(self.attn_head, self.attn_size, self.hidden_dim, self.hidden_dim, self.hidden_dim, self.dropout)\n",
    "        self.regression_input_size = self.attn_size + self.hidden_dim\n",
    "        self.regressor = self.make_regressor()\n",
    "        self.input_linear = nn.Linear(11,10)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        self.Norm_ = nn.LayerNorm(10, device = self.device)\n",
    "        self.r = Variable(nn.init.xavier_normal_(torch.empty(self.batch_size, 10,self.hidden_dim)), requires_grad=True).to(self.device)\n",
    "        self.b = Variable(torch.zeros(self.batch_size, 10,self.hidden_dim), requires_grad=True).to(self.device)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h = torch.empty(self.num_layers, self.batch_size, self.hidden_dim)\n",
    "        c = torch.empty(self.num_layers, self.batch_size, self.hidden_dim)\n",
    "        return (nn.init.xavier_normal_(h),\n",
    "                nn.init.xavier_normal_(c))\n",
    "\n",
    "    def make_regressor(self): # 간단한 MLP를 만드는 함수\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.regression_input_size))  ##  nn.BatchNorm1d\n",
    "        layers.append(nn.Dropout(self.dropout))    ##  nn.Dropout\n",
    "\n",
    "        ## hidden dim을 outputdim으로 바꿔주는 MLP\n",
    "        layers.append(nn.Linear(self.regression_input_size, self.hidden_dim))\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x_input: torch.Size([64, 10, 11])\n",
    "        x_output : torch.Size([10, 64, 10])  (seq_len, batch_size, feature)\n",
    "        lstm_out :  x : torch.Size([10, 64, 10]), hidden : torch.Size([1, 64, 10]), cell state : torch.Size([1, 64, 10])\n",
    "        '''\n",
    "\n",
    "        x = self.tanh1(self.input_linear(x.float())).transpose(0,1).float() \n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "       \n",
    "        # self.hidden # 0번째가 히든스테이트임 1번째는 cell state\n",
    "        # print(self.hidden[0].size()) # [1, 128, 10] # final hidden state\n",
    "        # print(lstm_out.size()) # [10, 128, 10] sequence length, batch size, feature length\n",
    "        hidden_context, attn_weights = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        ## lstm_out : 각 time step에서의 lstm 모델의 output 값\n",
    "        ## lstm_out[-1] : 맨마지막의 아웃풋 값으로 그 다음을 예측하고싶은 것이기 때문에 -1을 해줌\n",
    "        # hidden_context : torch.Size([64, 10, 10]) batch seq fea\n",
    "        hidden_context = self.r * self.Norm_(hidden_context) + self.b\n",
    "\n",
    "        return hidden_context, attn_weights\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2982973059bb32b2c180859bcc6f8f2be9d4decc2aea8640d27e050b49ac75fb"
  },
  "kernelspec": {
   "display_name": "py38_64",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
